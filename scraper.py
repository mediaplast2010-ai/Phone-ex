# scraper.py
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.chrome.service import Service
from webdriver_manager.chrome import ChromeDriverManager
from selenium.webdriver.common.by import By
import time
import csv
import os

# –°–ø–∏—Å–æ–∫ –º—ñ—Å—Ç (–º–æ–∂–Ω–∞ —Ä–æ–∑—à–∏—Ä–∏—Ç–∏)
CITIES = [
    "kiev", "lviv", "odessa", "kharkiv", "dnipro", "vinnytsia", "zhytomyr",
    "zaporizhzhia", "ivano-frankivsk", "ternopil", "poltava", "rovno",
    "sumy", "khmelnytskyi", "cherkasy", "chernivtsi", "chernihiv", "lutsk"
]

OUTPUT_FILE = "results.csv"

def create_driver():
    options = Options()
    options.add_argument("--start-maximized")
    # options.add_argument("--headless")  # –†–æ–∑–∫–æ–º–µ–Ω—Ç—É–π, —â–æ–± –ø—Ä–∏—Ö–æ–≤–∞—Ç–∏ –±—Ä–∞—É–∑–µ—Ä
    service = Service(ChromeDriverManager().install())
    return webdriver.Chrome(service=service, options=options)

def extract_phones_from_page(driver, city):
    phones = set()
    try:
        # –ü—Ä–æ–∫—Ä—É—Ç–∫–∞ –¥–æ –∫—ñ–Ω—Ü—è (–ø—ñ–¥–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è)
        last_height = driver.execute_script("return document.body.scrollHeight")
        while True:
            driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
            time.sleep(2)
            new_height = driver.execute_script("return document.body.scrollHeight")
            if new_height == last_height:
                break
            last_height = new_height

        # –ö–ª—ñ–∫–∞—î–º–æ –Ω–∞ –≤—Å—ñ "–ü–æ–∫–∞–∑–∞—Ç–∏ —Ç–µ–ª–µ—Ñ–æ–Ω"
        buttons = driver.find_elements(By.XPATH, "//*[contains(text(), '–ü–æ–∫–∞–∑–∞—Ç–∏ —Ç–µ–ª–µ—Ñ–æ–Ω') or contains(text(), '–ü–æ–∫–∞–∑–∞—Ç—å —Ç–µ–ª–µ—Ñ–æ–Ω')]")
        print(f"üîß –ö–ª—ñ–∫–∞—î–º–æ –Ω–∞ {len(buttons)} –∫–Ω–æ–ø–æ–∫ —É –º—ñ—Å—Ç—ñ {city}")
        for btn in buttons:
            try:
                driver.execute_script("arguments[0].click();", btn)
                time.sleep(0.2)
            except:
                continue

        time.sleep(2)

        # –ó–±–∏—Ä–∞—î–º–æ –≤—Å—ñ —Ç–µ–ª–µ—Ñ–æ–Ω–∏
        tel_links = driver.find_elements(By.XPATH, "//a[starts-with(@href, 'tel:')]")
        for link in tel_links:
            phone = link.get_attribute("href").replace("tel:", "").strip()
            if phone.startswith("380") or phone.startswith("0"):
                if phone.startswith("0") and len(phone) == 10:
                    phone = "38" + phone
                elif phone.startswith("0") and len(phone) == 9:
                    phone = "380" + phone
                phone = "+" + phone
            phones.add(phone)

    except Exception as e:
        print(f"–ü–æ–º–∏–ª–∫–∞ –ø—Ä–∏ –æ–±—Ä–æ–±—Ü—ñ —Å—Ç–æ—Ä—ñ–Ω–∫–∏: {e}")

    return phones

def run_scraper():
    driver = create_driver()
    with open(OUTPUT_FILE, "w", newline="", encoding="utf-8") as f:
        writer = csv.writer(f)
        writer.writerow(["–ú—ñ—Å—Ç–æ", "–¢–µ–ª–µ—Ñ–æ–Ω"])

        for city in CITIES:
            url = f"https://okna.ua/{city}/montazhniki"
            print(f"üåê –í—ñ–¥–∫—Ä–∏–≤–∞—î–º–æ: {url}")
            try:
                driver.get(url)
                time.sleep(5)  # –ß–∞—Å –Ω–∞ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è

                phones = extract_phones_from_page(driver, city)
                for phone in phones:
                    writer.writerow([city, phone])
                print(f"‚úÖ {city}: –∑–Ω–∞–π–¥–µ–Ω–æ {len(phones)} —Ç–µ–ª–µ—Ñ–æ–Ω—ñ–≤")

            except Exception as e:
                print(f"‚ùå –ü–æ–º–∏–ª–∫–∞ –∑ {city}: {e}")

            time.sleep(2)

    driver.quit()
    print(f"\nüéâ –ì–æ—Ç–æ–≤–æ! –£—Å—ñ —Ç–µ–ª–µ—Ñ–æ–Ω–∏ –∑–±–µ—Ä–µ–∂–µ–Ω–æ –≤ {OUTPUT_FILE}")